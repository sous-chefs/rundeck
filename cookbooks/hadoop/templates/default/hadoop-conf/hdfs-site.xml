<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<!-- file system properties -->

<property>
<name>dfs.name.dir</name>
<value><%= node[:hadoop][:dfs][:name_dir] %></value>
<description>Determines where on the local filesystem the DFS name node
should store the name table. If this is a comma-delimited list
of directories then the name table is replicated in all of the
directories, for redundancy. </description>
<final>true</final>
</property>

<property>
<name>dfs.data.dir</name>
<value><%= node[:hadoop][:dfs][:data_dir] %></value>
<description>Determines where on the local filesystem an DFS data node
should store its blocks. If this is a comma-delimited
list of directories, then data will be stored in all named
directories, typically on different devices.
Directories that do not exist are ignored.
</description>
<final>true</final>
</property>

<property>
<name>dfs.safemode.threshold.pct</name>
<value>1.0f</value>
<description>
Specifies the percentage of blocks that should satisfy
the minimal replication requirement defined by dfs.replication.min.
Values less than or equal to 0 mean not to start in safe mode.
Values greater than 1 will make safe mode permanent.
</description>
</property>

<property>
<name>dfs.datanode.address</name>
<value>0.0.0.0:50010</value>
</property>

<property>
<name>dfs.datanode.http.address</name>
<value>0.0.0.0:50075</value>
</property>

<property>
<name>dfs.http.address</name>
<value><%= @namenode %>:50070</value>
<description>The name of the default file system. Either the
literal string "local" or a host:port for NDFS.
</description>
<final>true</final>
</property>

<property>
<name>dfs.web.ugi</name>
<value>hadoop,hadoop</value>
<final>true</final>
</property>
	
<!-- Permissions configuration -->
<property>
<name>dfs.umaskmode</name>
<value>077</value>
<description>
The octal umask used when creating files and directories.
</description>
</property>

<property>
<name>dfs.block.access.token.enable</name>
<value>false</value>
<description>
Are access tokens are used as capabilities for accessing datanodes.
</description>
</property>

<property>
<name>dfs.namenode.kerberos.principal</name>
<value>nn/_HOST@${local.realm}</value>
<description>
Kerberos principal name for the NameNode
</description>
</property>

<property>
<name>dfs.secondary.namenode.kerberos.principal</name>
<value>nn/_HOST@${local.realm}</value>
<description>
Kerberos principal name for the secondary NameNode.
</description>
</property>


<property>
<name>dfs.namenode.kerberos.https.principal</name>
<value>host/_HOST@${local.realm}</value>
<description>
The Kerberos principal for the host that the NameNode runs on.
</description>
</property>

<property>
<name>dfs.secondary.namenode.kerberos.https.principal</name>
<value>host/_HOST@${local.realm}</value>
<description>
The Kerberos principal for the hostthat the secondary NameNode runs on.
</description>
</property>

<property>
<name>dfs.secondary.https.port</name>
<value>50490</value>
<description>The https port where secondary-namenode binds</description>

</property>

<property>
<name>dfs.datanode.kerberos.principal</name>
<value>dn/_HOST@${local.realm}</value>
<description>
The Kerberos principal that the DataNode runs as. "_HOST" is replaced by
the real host name.
</description>
</property>

<property>
<name>dfs.web.authentication.kerberos.principal</name>
<value>HTTP/_HOST@${local.realm}</value>
<description>
The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.

The HTTP Kerberos principal MUST start with 'HTTP/' per Kerberos
HTTP SPENGO specification.
</description>
</property>

<property>
<name>dfs.web.authentication.kerberos.keytab</name>
<value>/etc/security/keytabs/nn.service.keytab</value>
<description>
The Kerberos keytab file with the credentials for the
HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.
</description>
</property>

<property>
<name>dfs.namenode.keytab.file</name>
<value>/etc/security/keytabs/nn.service.keytab</value>
<description>
Combined keytab file containing the namenode service and host principals.
</description>
</property>

<property>
<name>dfs.secondary.namenode.keytab.file</name>
<value>/etc/security/keytabs/nn.service.keytab</value>
<description>
Combined keytab file containing the namenode service and host principals.
</description>
</property>

<property>
<name>dfs.datanode.keytab.file</name>
<value>/etc/security/keytabs/dn.service.keytab</value>
<description>
The filename of the keytab file for the DataNode.
</description>
</property>

<property>
<name>dfs.https.port</name>
<value>50470</value>
<description>The https port where namenode binds</description>
</property>

<property>
<name>dfs.https.address</name>
<value><%= @namenode %>:50470</value>
<description>The https address where namenode binds</description>
</property>

<property>
<name>dfs.datanode.data.dir.perm</name>
<value>700</value>
<description>The permissions that should be there on dfs.data.dir
directories. The datanode will not come up if the permissions are
different on existing dfs.data.dir directories. If the directories
don't exist, they will be created with this permission.
</description>
</property>

<property>
<name>dfs.cluster.administrators</name>
<value>hdfs</value>
<description>ACL for who all can view the default servlets in the HDFS</description>
</property>

<property>
<name>dfs.permissions.superusergroup</name>
<value>hadoop</value>
<description>The name of the group of super-users.</description>
</property>

<property>
<name>dfs.secondary.http.address</name>
<value><%= @backupnamenode %>:50090</value>
<description>
The secondary namenode http server address and port.
If the port is 0 then the server will start on a free port.
</description>
</property>

<property>
<name>dfs.hosts</name>
<value>/etc/hadoop/dfs.include</value>
<description>Names a file that contains a list of hosts that are
permitted to connect to the namenode. The full pathname of the file
must be specified. If the value is empty, all hosts are
permitted.</description>
</property>

<property>
<name>dfs.hosts.exclude</name>
<value>/etc/hadoop/dfs.exclude</value>
<description>Names a file that contains a list of hosts that are
not permitted to connect to the namenode. The full pathname of the
file must be specified. If the value is empty, no hosts are
excluded.
</description>
</property>
<property>
<name>dfs.webhdfs.enabled</name>
<value>false</value>
<description>Enable or disable webhdfs. Defaults to false</description>
</property>
<property>
<name>dfs.support.append</name>
<value>true</value>
<description>Enable or disable append. Defaults to false</description>
</property>

<property>
<name>dfs.block.size</name>
<value><%= node[:hadoop][:dfs][:block_size] %></value>
<description>The default block size for new files.</description>
<final>true</final>
</property>


<property>
<name>dfs.datanode.handler.count</name>
<value><%= node[:hadoop][:dfs][:datanode_handler_count] %></value>
<description>The number of server threads for the namenode.</description>
<final>true</final>
</property>

<property>
<name>dfs.namenode.handler.count</name>
<value><%= node[:hadoop][:dfs][:namenode_handler_count] %></value>
<description>The number of server threads for the namenode.</description>
<final>true</final>
</property>

<property>
<name>dfs.datanode.du.reserved</name>
<value><%= node[:hadoop][:dfs][:namenode_du_reserved] %></value>
<description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.</description>
<final>true</final>
</property>

<property>
<name>dfs.replication</name>
<value><%= node[:hadoop][:dfs][:replication] %></value>
<description>Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.</description>
<final>true</final>
</property>

<property>
<name>dfs.permissions</name>
<value><%= node[:hadoop][:dfs][:permissions] %></value>
<description>If "true", enable permission checking in HDFS. If "false", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories.</description>
<final>true</final>
</property>

<property>
<name>dfs.datanode.max.xcievers</name>
<value><%= node[:hadoop][:dfs][:datanode_max_xcievers] %></value>
<description>n/a</description>
<final>true</final>
</property>


</configuration>





